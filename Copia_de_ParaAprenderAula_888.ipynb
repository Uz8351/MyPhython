{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18QN4xmJ1BYA5mMqunimVU0VMy2EClCDq",
      "authorship_tag": "ABX9TyNpFgUGGOHSY1NqmACEj0UC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uz8351/MyPhython/blob/master/Copia_de_ParaAprenderAula_888.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF4VZ2FKlJMF"
      },
      "outputs": [],
      "source": [
        "# importando los recursos\n",
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import requests\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# obtiene la porción \"features\" de VGG19\n",
        "vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "# congelamos todos los parámetros VGG dado que solo estamos optimizando la imagen objetivo\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad_(False)\n",
        "# mover el modelo a GPU si está disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "vgg.to(device)\n",
        "def load_image(img_path, max_size=400, shape=None):\n",
        "    ''' Load in and transform an image, making sure the image\n",
        "       is <= 400 pixels in the x-y dims.'''\n",
        "    if \"http\" in img_path:\n",
        "        response = requests.get(img_path)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    # large images will slow down processing\n",
        "    if max(image.size) > max_size:\n",
        "        size = max_size\n",
        "    else:\n",
        "        size = max(image.size)\n",
        "\n",
        "    if shape is not None:\n",
        "        size = shape\n",
        "\n",
        "    in_transform = transforms.Compose([\n",
        "                        transforms.Resize(size),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                                             (0.229, 0.224, 0.225))])\n",
        "\n",
        "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
        "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
        "\n",
        "    return image\n",
        "# cargando las imágenes content y style\n",
        "content = load_image('/content/drive/MyDrive/druida/Cat/0.jpg').to(device)\n",
        "# Redimensionar style para emparejar al content, hace más facil el trabajo\n",
        "style = load_image('/content/drive/MyDrive/druida/Dog/1.jpg', shape=content.shape[-2:]).to(device)\n",
        "# función para desnormalizar una imagen\n",
        "# y convertirla de una imagen Tensor a una imagen NumPy para su visualización\n",
        "def im_convert(tensor):\n",
        "    \"\"\" Muestra un tensor como una imagen. \"\"\"\n",
        "\n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.numpy().squeeze()\n",
        "    image = image.transpose(1,2,0)\n",
        "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "    image = image.clip(0, 1)\n",
        "\n",
        "    return image\n",
        "# muestra la imagen\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))\n",
        "# content y style lado a lado\n",
        "plt.axis('off')\n",
        "\n",
        "ax1.axes.get_xaxis().set_visible(False)\n",
        "ax1.axes.get_yaxis().set_visible(False)\n",
        "ax1.imshow(im_convert(content))\n",
        "\n",
        "ax2.axes.get_xaxis().set_visible(False)\n",
        "ax2.axes.get_yaxis().set_visible(False)\n",
        "ax2.imshow(im_convert(style))\n",
        "def get_features(image, model, layers=None):\n",
        "    \"\"\" Correr una imagen hacia adelante a través de un modelo y obtener las características para\n",
        "        un conjunto de capas. Las capas predeterminadas son para VGGNet que coincida con Gatys et al (2016)\n",
        "    \"\"\"\n",
        "\n",
        "    if layers is None:\n",
        "        layers = {'0': 'conv1_1',\n",
        "                  '5': 'conv2_1',\n",
        "                  '10': 'conv3_1',\n",
        "                  '19': 'conv4_1',\n",
        "                  '21': 'conv4_2',\n",
        "                  '28': 'conv5_1'}\n",
        "\n",
        "    features = {}\n",
        "    x = image\n",
        "    # model._modules es un diccionario que contiene cada módule en el modelo\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "\n",
        "    return features\n",
        "def gram_matrix(tensor):\n",
        "\n",
        "\n",
        "    # obtiene el batch_size, profundidad, alto y ancho del Tensor\n",
        "    _, d, h, w = tensor.size()\n",
        "\n",
        "    # redimensiona para poder multiplicar los features para cada canal\n",
        "    tensor = tensor.view(d, h * w)\n",
        "\n",
        "    # calcula la gram matrix\n",
        "    gram = torch.mm(tensor, tensor.t())\n",
        "\n",
        "    return gram\n",
        "# obtiene los features de content y style features  una sola vez antes de entrenar\n",
        "content_features = get_features(content, vgg)\n",
        "style_features = get_features(style, vgg)\n",
        "\n",
        "# calcula las matrices Gram para cada capa de nuestra representación de estilo\n",
        "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "# crea una tercera imagen, nuestra imagen objetivo/target, y la prepara para que cambie\n",
        "\n",
        "target = content.clone().requires_grad_(True).to(device)\n",
        "# weights para cada capa de estilo\n",
        "# note que excluimos `conv4_2` de nuestra representación de contenido\n",
        "style_weights = {'conv1_1': 1.,\n",
        "                 'conv2_1': 0.75,\n",
        "                 'conv3_1': 0.2,\n",
        "                 'conv4_1': 0.2,\n",
        "                 'conv5_1': 0.2}\n",
        "\n",
        "content_weight = 1  # alpha\n",
        "style_weight = 1e6  # beta\n",
        "#content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
        "# cada cuantos pasos se mostrará un avance\n",
        "show_every = 250\n",
        "\n",
        "# hyperparametros de iteración\n",
        "optimizer = optim.Adam([target], lr=0.003)\n",
        "steps = 5000  # cuantas iteraciónes tomará la imagen\n",
        "\n",
        "for ii in range(1, steps+1):\n",
        "\n",
        "    # obtiene los features de la imagen objetivo/target\n",
        "    target_features = get_features(target, vgg)\n",
        "\n",
        "    # la perdida de contenido\n",
        "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
        "\n",
        "    # la perdida de estilo\n",
        "    # inicializa en 0\n",
        "    style_loss = 0\n",
        "    # luego se suma cada pérdida de capa de matriz Gram\n",
        "    for layer in style_weights:\n",
        "        # obtiene la representación de estilo de la imagen objetivo para la capa\n",
        "        target_feature = target_features[layer]\n",
        "        target_gram = gram_matrix(target_feature)\n",
        "        _, d, h, w = target_feature.shape\n",
        "        # obtiene la representación de estilo\n",
        "        style_gram = style_grams[layer]\n",
        "        # la pérdida de estilo para una capa multiplicada por su beta\n",
        "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
        "        # suma las pérdidas de estilo\n",
        "        style_loss += layer_style_loss / (d * h * w)\n",
        "\n",
        "    # calcula la pérdida total\n",
        "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "\n",
        "    # actualiza la imagen objetivo\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # muestra las imágenes intermedias e imprime la pérdida y el número de iteración\n",
        "    if  ii % show_every == 0:\n",
        "        print('Pérdida total / Total loss: ', total_loss.item())\n",
        "        print('Iteración # ', ii)\n",
        "        plt.imshow(im_convert(target))\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "# Muestra la imagen original (content) y la imagen final (target)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))\n",
        "ax1.axes.get_xaxis().set_visible(False)\n",
        "ax1.axes.get_yaxis().set_visible(False)\n",
        "ax1.imshow(im_convert(content))\n",
        "\n",
        "ax2.axes.get_xaxis().set_visible(False)\n",
        "ax2.axes.get_yaxis().set_visible(False)\n",
        "ax2.imshow(im_convert(target))\n"
      ]
    }
  ]
}